{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "\"\"\"\n",
        "# Week 2 Day 1 - Build Your Own BERT\n",
        "Today you'll implement your own BERT model such that it can load the weights from the actual BERT and predict some masked tokens.\n",
        "Reading: [Language Modelling with Transformers](https://docs.google.com/document/d/1XJQT8PJYzvL0CLacctWcT0T5NfL7dwlCiIqRtdTcIqA/edit#)\n",
        "Reading: [BERT Paper, Section 3.1 \"Pre-Training BERT\"](https://arxiv.org/pdf/1810.04805.pdf)\n",
        "Refer to the below schematic for the architecture of BERT. You can ignore the classification head\n",
        "\n",
        "See here for BERT architecture:\n",
        "https://i.imgur.com/2ekVyly.png\n",
        "\n",
        "If this is too hard, I'd recommend\n",
        "(foundations) getting familiar with classes here https://realpython.com/python3-object-oriented-programming/\n",
        "(building on this) understand what the point of modules is: https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html (classes, that can be nested, and always implement a forward method, that takes in a tensor and returns a tensor)\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the EasyTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.system(\"pip install transformers einops fancy_einsum\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cell just makes utilities for tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import os\n",
        "import time\n",
        "import torch as t\n",
        "import transformers\n",
        "import requests\n",
        "import logging\n",
        "import http\n",
        "from functools import wraps\n",
        "from transformers.models.bert.modeling_bert import BertForMaskedLM\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2LMHeadModel\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_pretrained_gpt() -> GPT2LMHeadModel:\n",
        "    \"\"\"Load the HuggingFace GPT-2.\n",
        "\n",
        "    On first use this downloads about 500MB from the Internet.\n",
        "    Later uses should hit the cache and take under 1s to load.\n",
        "    \"\"\"\n",
        "    return transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "\n",
        "\n",
        "def load_pretrained_bert() -> BertForMaskedLM:\n",
        "    \"\"\"Load the HuggingFace BERT.\n",
        "\n",
        "    Supresses the spurious warning about some weights not being used.\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger(\"transformers.modeling_utils\")\n",
        "    was_disabled = logger.disabled\n",
        "    logger.disabled = True\n",
        "    bert = transformers.BertForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
        "    logger.disabled = was_disabled\n",
        "    return bert\n",
        "\n",
        "\n",
        "def assert_all_equal(actual: t.Tensor, expected: t.Tensor):\n",
        "    mask = actual == expected\n",
        "    if not mask.all():\n",
        "        bad = mask.nonzero()\n",
        "        msg = f\"Did not match at {len(bad)} indexes: {bad[:10]}{'...' if len(bad) > 10 else ''}\"\n",
        "        raise AssertionError(f\"{msg}\\nActual:\\n{actual}\\nExpected:\\n{expected}\")\n",
        "\n",
        "\n",
        "def test_is_equal(actual: t.Tensor, expected: t.Tensor, test_name: str):\n",
        "    try:\n",
        "        run_and_report(assert_all_equal, test_name, actual, expected)\n",
        "    except AssertionError as e:\n",
        "        print(f\"Test failed: {test_name}\")\n",
        "        raise e\n",
        "\n",
        "\n",
        "def assert_shape_equal(actual: t.Tensor, expected: t.Tensor):\n",
        "    if actual.shape != expected.shape:\n",
        "        raise AssertionError(f\"expected shape={expected.shape}, got {actual.shape}\")\n",
        "\n",
        "\n",
        "def allclose(actual: t.Tensor, expected: t.Tensor, rtol=1e-4, atol: Optional[float] = None) -> None:\n",
        "    if (rtol is None) == (atol is None):\n",
        "        raise Exception(\"This version of allclose expects exactly one of rtol and atol\")\n",
        "\n",
        "    assert_shape_equal(actual, expected)\n",
        "\n",
        "    left = (actual - expected).abs()\n",
        "    if rtol is not None:\n",
        "        right = rtol * expected.abs()\n",
        "        pct_wrong = int(100 * (left > right).float().mean())\n",
        "    elif atol is not None:\n",
        "        pct_wrong = int(100 * (left > atol).float().mean())\n",
        "    else:\n",
        "        raise Exception(\"Bad arguments\")\n",
        "\n",
        "    if pct_wrong > 0:\n",
        "        print(f\"Test failed. Max absolute deviation: {left.max()}\")\n",
        "        print(f\"Actual:\\n{actual}\\nExpected:\\n{expected}\")\n",
        "        raise AssertionError(f\"allclose failed with {pct_wrong} percent of entries outside tolerance\")\n",
        "\n",
        "\n",
        "def allclose_atol(actual: t.Tensor, expected: t.Tensor, atol: float) -> None:\n",
        "    assert_shape_equal(actual, expected)\n",
        "    left = (actual - expected).abs()\n",
        "    pct_wrong = int(100 * (left > atol).float().mean())\n",
        "    if pct_wrong > 0:\n",
        "        print(f\"Test failed. Max absolute deviation: {left.max()}\")\n",
        "        print(f\"Actual:\\n{actual}\\nExpected:\\n{expected}\")\n",
        "        raise AssertionError(f\"allclose failed with {pct_wrong} percent of entries outside tolerance\")\n",
        "\n",
        "\n",
        "def allclose_scalar(actual: float, expected: float, rtol=1e-4, atol: Optional[float] = None) -> None:\n",
        "    if (rtol is None) == (atol is None):\n",
        "        raise Exception(\"This version of allclose expects exactly one of rtol and atol\")\n",
        "    left = abs(actual - expected)\n",
        "    if rtol is not None:\n",
        "        right = rtol * abs(expected)\n",
        "        wrong = left > right\n",
        "    elif atol is not None:\n",
        "        wrong = left > atol\n",
        "    else:\n",
        "        raise Exception(\"Bad arguments\")\n",
        "\n",
        "    if wrong:\n",
        "        print(f\"Test failed. Absolute deviation: {left}\")\n",
        "        print(f\"Actual:\\n{actual}\\nExpected:\\n{expected}\")\n",
        "\n",
        "\n",
        "def report_success(testname):\n",
        "    \"\"\"POST to the server indicating success at the given test.\n",
        "\n",
        "    Used to help the TAs know how long each section takes to complete.\n",
        "    \"\"\"\n",
        "    server = os.environ.get(\"MLAB_SERVER\")\n",
        "    email = os.environ.get(\"MLAB_EMAIL\")\n",
        "    if server:\n",
        "        if email:\n",
        "            r = requests.post(\n",
        "                server + \"/api/report_success\",\n",
        "                json=dict(email=email, testname=testname),\n",
        "            )\n",
        "            if r.status_code != http.HTTPStatus.NO_CONTENT:\n",
        "                raise ValueError(f\"Got status code from server: {r.status_code}\")\n",
        "        else:\n",
        "            raise ValueError(f\"Server set to {server} but no MLAB_EMAIL set!\")\n",
        "    else:\n",
        "        if email:\n",
        "            raise ValueError(f\"Email set to {email} but no MLAB_SERVER set!\")\n",
        "        else:\n",
        "            return  # local dev, do nothing\n",
        "\n",
        "\n",
        "# Map from qualified name \"test_w2d3.test_unidirectional_attn\" to whether this test was passed in the current interpreter session\n",
        "# Note this can get clobbered during autoreload\n",
        "TEST_FN_PASSED = {}\n",
        "\n",
        "\n",
        "def report(test_func):\n",
        "    name = f\"{test_func.__module__}.{test_func.__name__}\"\n",
        "    # This can happen when using autoreload, so don't complain about it.\n",
        "    # if name in TEST_FN_PASSED:\n",
        "    #     raise KeyError(f\"Already registered: {name}\")\n",
        "    TEST_FN_PASSED[name] = False\n",
        "\n",
        "    @wraps(test_func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        return run_and_report(test_func, name, *args, **kwargs)\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "def run_and_report(test_func, name, *test_func_args, **test_func_kwargs):\n",
        "    start = time.time()\n",
        "    out = test_func(*test_func_args, **test_func_kwargs)\n",
        "    elapsed = time.time() - start\n",
        "    print(f\"{name} passed in {elapsed:.2f}s.\")\n",
        "    if not TEST_FN_PASSED.get(name):\n",
        "        report_success(name)\n",
        "        TEST_FN_PASSED[name] = True\n",
        "    return out\n",
        "\n",
        "\n",
        "def remove_hooks(module: t.nn.Module):\n",
        "    \"\"\"Remove all hooks from module.\n",
        "\n",
        "    Use module.apply(remove_hooks) to do this recursively.\n",
        "    \"\"\"\n",
        "    module._backward_hooks.clear()\n",
        "    module._forward_hooks.clear()\n",
        "    module._forward_pre_hooks.clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Setup the BERT objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Union\n",
        "\n",
        "\n",
        "import torch as t\n",
        "import transformers\n",
        "from einops import rearrange, repeat\n",
        "from fancy_einsum import einsum\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class BertConfig:\n",
        "    \"\"\"Constants used throughout the Bert model. Most are self-explanatory.\n",
        "    intermediate_size is the number of hidden neurons in the MLP (see schematic)\n",
        "    type_vocab_size is only used for pretraining on \"next sentence prediction\", which we aren't doing.\n",
        "    \"\"\"\n",
        "\n",
        "    vocab_size: int = 28996\n",
        "    intermediate_size: int = 3072\n",
        "    hidden_size: int = 768\n",
        "    num_layers: int = 12\n",
        "    num_heads: int = 12\n",
        "    max_position_embeddings: int = 512\n",
        "    dropout: float = 0.1\n",
        "    type_vocab_size: int = 2\n",
        "    layer_norm_epsilon: float = 1e-12\n",
        "\n",
        "\n",
        "config = BertConfig()\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class BertOutput:\n",
        "    \"\"\"The output of your Bert model.\n",
        "    logits is used for W2D1 and is the prediction for each token in the vocabulary.\n",
        "    The other fields are used on W2D2 for the sentiment task.\n",
        "    \"\"\"\n",
        "\n",
        "    logits: Optional[t.Tensor] = None\n",
        "    is_positive: Optional[t.Tensor] = None\n",
        "    star_rating: Optional[t.Tensor] = None\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the exercises begin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# %%\n",
        "\"\"\"\n",
        "# Embedding`\n",
        "Implement your version of PyTorch's `nn.Embedding` module. The PyTorch version has some extra options in the constructor, but you don't need to implement those since BERT doesn't use them.\n",
        "The `Parameter` should be named `weight` and initialized with normally distributed random values.\n",
        "\"\"\"\n",
        "# %%\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(t.randn(num_embeddings, embedding_dim))\n",
        "\n",
        "    def forward(self, x: t.LongTensor) -> t.Tensor:\n",
        "        \"\"\"For each integer in the input, return that row of the embedding.\n",
        "        Don't convert x to one-hot vectors - this works but is too slow.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "def test_embedding(Embedding):\n",
        "    \"\"\"Indexing into the embedding should fetch the corresponding rows of the embedding.\"\"\"\n",
        "    emb = Embedding(6, 10)\n",
        "    out = emb(t.LongTensor([1, 3, 5]))\n",
        "    allclose(out[0], emb.weight[1])\n",
        "    allclose(out[1], emb.weight[3])\n",
        "    allclose(out[2], emb.weight[5])\n",
        "\n",
        "test_embedding(Embedding)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# %%\n",
        "\"\"\"\n",
        "# Layer Normalization\n",
        "Use the ([PyTorch docs](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)) for Layer Normalization to implement your own version which exactly mimics the official API. Use the biased estimator for Var[x] as shown in the docs. You can assume elementwise affine is always True.\n",
        "\"\"\"\n",
        "# %%\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(\n",
        "        self, normalized_shape: Union[int, tuple, t.Size], eps=1e-5, elementwise_affine=True, device=None, dtype=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.weight = None\n",
        "        self.bias = None # implement these\n",
        "\n",
        "    def forward(self, x: t.Tensor):\n",
        "        \"\"\"x and the output should both have shape (N, *).\"\"\"\n",
        "        pass\n",
        "\n",
        "def test_layernorm_mean_1d(LayerNorm):\n",
        "    \"\"\"If an integer is passed, this means normalize over the last dimension which should have that size.\"\"\"\n",
        "    x = t.randn(20, 10)\n",
        "    ln1 = LayerNorm(10)\n",
        "    out = ln1(x)\n",
        "    max_mean = out.mean(-1).abs().max().item()\n",
        "    assert max_mean < 1e-5, f\"Normalized mean should be about 0, got {max_mean}\"\n",
        "\n",
        "\n",
        "\n",
        "def test_layernorm_mean_2d(LayerNorm):\n",
        "    \"\"\"If normalized_shape is 2D, should normalize over both the last two dimensions.\"\"\"\n",
        "    x = t.randn(20, 10)\n",
        "    ln1 = LayerNorm((20, 10))\n",
        "    out = ln1(x)\n",
        "    max_mean = out.mean((-1, -2)).abs().max().item()\n",
        "    assert max_mean < 1e-5, f\"Normalized mean should be about 0, got {max_mean}\"\n",
        "\n",
        "\n",
        "\n",
        "def test_layernorm_std(LayerNorm):\n",
        "    \"\"\"If epsilon is small enough and no elementwise_affine, the output variance should be very close to 1.\"\"\"\n",
        "    x = t.randn(20, 10)\n",
        "    ln1 = LayerNorm(10, eps=1e-11, elementwise_affine=False)\n",
        "    out = ln1(x)\n",
        "    var_diff = (1 - out.var(-1, unbiased=False)).abs().max().item()\n",
        "    assert var_diff < 1e-6, f\"Var should be about 1, off by {var_diff}\"\n",
        "\n",
        "\n",
        "\n",
        "def test_layernorm_exact(LayerNorm):\n",
        "    \"\"\"Your LayerNorm's output should exactly match PyTorch for equal epsilon.\"\"\"\n",
        "    x = t.randn(2, 3, 4, 5)\n",
        "    # Use large epsilon to make sure it fails if they forget it\n",
        "    ln1 = LayerNorm((5,), eps=1e-2)\n",
        "    ln2 = t.nn.LayerNorm((5,), eps=1e-2)  # type: ignore\n",
        "    actual = ln1(x)\n",
        "    expected = ln2(x)\n",
        "    allclose(actual, expected)\n",
        "\n",
        "def test_layernorm_backward(LayerNorm):\n",
        "    \"\"\"The backwards pass should also match PyTorch exactly.\"\"\"\n",
        "    x = t.randn(10, 3)\n",
        "    x2 = x.clone()\n",
        "    x.requires_grad_(True)\n",
        "    x2.requires_grad_(True)\n",
        "\n",
        "    # Without parameters, should be deterministic\n",
        "    ref = nn.LayerNorm(3, elementwise_affine=False)\n",
        "    ref.requires_grad_(True)\n",
        "    ref(x).sum().backward()\n",
        "\n",
        "    ln = LayerNorm(3, elementwise_affine=False)\n",
        "    ln.requires_grad_(True)\n",
        "    ln(x2).sum().backward()\n",
        "    # Use atol since grad entries are supposed to be zero here\n",
        "    assert isinstance(x.grad, t.Tensor)\n",
        "    assert isinstance(x2.grad, t.Tensor)\n",
        "    allclose_atol(x.grad, x2.grad, atol=1e-5)\n",
        "\n",
        "test_layernorm_mean_1d(LayerNorm)\n",
        "test_layernorm_mean_2d(LayerNorm)\n",
        "test_layernorm_std(LayerNorm)\n",
        "test_layernorm_exact(LayerNorm)\n",
        "test_layernorm_backward(LayerNorm) # optional extra test for the backward pass - we aren't doing training so don't worry if it fails\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# %%\n",
        "\"\"\"\n",
        "# BertMLP\n",
        "Make the MLP block, following the schematic. Use `nn.Dropout` for the dropout layer.\n",
        "\"\"\"\n",
        "# %%\n",
        "\n",
        "class BertMLP(nn.Module):\n",
        "    def __init__(self, config: BertConfig):\n",
        "        super().__init__()\n",
        "        pass # refer to the diagram!!!\n",
        "\n",
        "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "        pass\n",
        "\n",
        "\n",
        "def test_bert_mlp(BertMLP, batch_size=2, seq_len=5, hidden_size=6, dropout=0.0):\n",
        "    \"\"\"The MLP's output should exactly match the reference solution.\n",
        "    Dropout is not tested.\n",
        "    \"\"\"\n",
        "\n",
        "    config = BertConfig(\n",
        "        hidden_size=hidden_size,\n",
        "        intermediate_size=3 * hidden_size,\n",
        "        dropout=dropout,\n",
        "    )\n",
        "    x = t.randn(batch_size, seq_len, hidden_size)\n",
        "\n",
        "    t.manual_seed(988)\n",
        "    ref = BertMLP(config)\n",
        "    expected = ref(x)\n",
        "\n",
        "    t.manual_seed(988)\n",
        "    yours = BertMLP(config)\n",
        "    actual = yours(x)\n",
        "\n",
        "    allclose(actual, expected)\n",
        "\n",
        "test_bert_mlp(BertMLP, dropout=0.5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# %%\n",
        "\"\"\"\n",
        "# Batched Self-Attention\n",
        "We're going to implement a version of self-attention that computes all sequences in a batch at once, and all heads at once. Make sure you understand how single sequence, single head attention works first.\n",
        "# Attention Pattern Pre-Softmax\n",
        "Spend at least 5 minutes thinking about how to batch the computation before looking at the spoilers.\n",
        "<details>\n",
        "<summary>What should the shape of `project_query` be?</summary>\n",
        "`project_query` should go from `hidden_size` to `num_heads * self.head_size` which in this case is equal to `hidden_size`. This represents all the heads's Q matrices concatenated together, and one call to it now computes all the queries at once (broadcasting over the leading batch and seq dimensions of the input x).\n",
        "</details>\n",
        "<details>\n",
        "<summary>Should my Linear layers have a bias?</summary>\n",
        "While these Linear layers are traditionally referred to as projections, in BERT they DO have a bias.\n",
        "</details>\n",
        "<details>\n",
        "<summary>What does the einsum to make the attention pattern look like?</summary>\n",
        "We need to sum out the head_size and keep the seq_q dimension before the seq_k dimension. For a single batch and single head, it would be: `einsum(\"seq_q head_size, seq_k head_size -> seq_q seq_k\")`. You'll want to do a `rearrange` before your `einsum`.\n",
        "</details>\n",
        "<details>\n",
        "<summary>Which dimension do I softmax over?</summary>\n",
        "The desired property is that `pattern[batch,head,q]` sums to 1 for all `q`. So the softmax needs to be over the `k` dimension.\n",
        "</details>\n",
        "<details>\n",
        "<summary>I'm still confused about how to batch the computation.</summary>\n",
        "## Pre Softmax\n",
        "- Apply Q, K, and V to the input x\n",
        "- rearrange Q and K to split the `hidden_size` dimension apart into heads and head_size dimensions.\n",
        "- Einsum Q and K to get a (batch, head, seq_q, seq_k) shape. \n",
        "- Divide by the square root of the head size.\n",
        "## Forward\n",
        "- Softmax over the `k` dimension to obtain attention probs\n",
        "- rearrange V just like Q and K previously\n",
        "- einsum V and your attention probs to get the weighted Vs\n",
        "- rearrange weighted Vs to combine head and head_size and put that at the end\n",
        "- apply O\n",
        "</details>\n",
        "# Attention Forward Function\n",
        "Your forward should call `attention_pattern_pre_softmax` and then finish the computations using `einsum` and `rearrange` again. Remember to apply the output projection.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config: BertConfig):\n",
        "        super().__init__()\n",
        "        self.num_heads = config.num_heads\n",
        "        assert config.hidden_size % config.num_heads == 0\n",
        "        self.head_size = config.hidden_size // config.num_heads\n",
        "        self.project_query = nn.Linear(config.hidden_size, config.num_heads * self.head_size)\n",
        "        self.project_key = nn.Linear(config.hidden_size, config.num_heads * self.head_size)\n",
        "        self.project_value = nn.Linear(config.hidden_size, config.num_heads * self.head_size)\n",
        "        self.project_output = nn.Linear(config.num_heads * self.head_size, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def attention_pattern_pre_softmax(self, x: t.Tensor) -> t.Tensor:\n",
        "        \"\"\"Return the attention pattern after scaling but before softmax.\n",
        "        pattern[batch, head, q, k] should be the match between a query at sequence position q and a key at sequence position k.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "        pass\n",
        "\n",
        "\n",
        "def test_attention_pattern_pre_softmax(BertSelfAttention, batch_size=2, seq_len=5, hidden_size=6, num_heads=2):\n",
        "    \"\"\"The attention pattern should exactly match the reference solution.\"\"\"\n",
        "\n",
        "    config = BertConfig(hidden_size=hidden_size, num_heads=num_heads)\n",
        "\n",
        "    x = t.randn(batch_size, seq_len, hidden_size)\n",
        "\n",
        "    t.manual_seed(987)\n",
        "    ref = BertSelfAttention(config)\n",
        "    expected = ref.attention_pattern_pre_softmax(x)\n",
        "\n",
        "    t.manual_seed(987)\n",
        "    yours = BertSelfAttention(config)\n",
        "    actual = yours.attention_pattern_pre_softmax(x)\n",
        "\n",
        "    allclose(actual, expected)\n",
        "\n",
        "\n",
        "\n",
        "def test_attention(BertSelfAttention, batch_size=2, seq_len=5, hidden_size=6, num_heads=2):\n",
        "    \"\"\"The attention layer's output should exactly match the reference solution.\n",
        "\n",
        "    Dropout is not tested!\n",
        "    \"\"\"\n",
        "\n",
        "    config = BertConfig(hidden_size=hidden_size, num_heads=num_heads, dropout=0.0)\n",
        "    x = t.randn(batch_size, seq_len, hidden_size)\n",
        "\n",
        "    t.manual_seed(988)\n",
        "    ref = BertSelfAttention(config)\n",
        "    expected = ref(x)\n",
        "\n",
        "    t.manual_seed(988)\n",
        "    yours = BertSelfAttention(config)\n",
        "    actual = yours(x)\n",
        "\n",
        "    allclose(actual, expected)\n",
        "\n",
        "test_attention_pattern_pre_softmax(BertSelfAttention)\n",
        "test_attention(BertSelfAttention)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# %%\n",
        "\"\"\"\n",
        "# Bert Block\n",
        "Assemble the BertAttention and BertBlock classes following the schematic.\n",
        "\"\"\"\n",
        "# %%\n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config: BertConfig):\n",
        "        super().__init__()\n",
        "        pass\n",
        "\n",
        "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "        pass\n",
        "\n",
        "\n",
        "class BertBlock(nn.Module):\n",
        "    def __init__(self, config: BertConfig):\n",
        "        super().__init__()\n",
        "        pass\n",
        "\n",
        "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "        pass\n",
        "\n",
        "\n",
        "def test_bert_block(BertBlock):\n",
        "    \"\"\"Your BertBlock should exactly match the reference solution in eval mode.\n",
        "\n",
        "    Dropout is not tested.\n",
        "    \"\"\"\n",
        "\n",
        "    config = BertConfig()\n",
        "    t.random.manual_seed(0)\n",
        "    reference = BertBlock(config)\n",
        "    reference.eval()\n",
        "    t.random.manual_seed(0)\n",
        "    theirs = BertBlock(config)\n",
        "    theirs.eval()\n",
        "    input_activations = t.rand((2, 3, 768))\n",
        "    allclose(theirs(input_activations), reference(input_activations))\n",
        "\n",
        "test_bert_block(BertBlock)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# %%\n",
        "\"\"\"\n",
        "# Putting it All Together\n",
        "Now fill in the entire Bert module, following the schematic. Tips:\n",
        "- The language modelling `Linear` after the blocks has shape `(embedding_size, embedding_size)`\n",
        "- If `token_type_ids` isn't provided to `forward`, make it the same shape as `input_ids` but all zeros.\n",
        "- The unembedding at the end that takes data from `hidden_size` to `vocab_size` shouldn't be its own Linear layer because it shares the same data as `token_embedding.weight`. Just reuse `token_embedding.weight` and add a bias term.\n",
        "- Print your model out to see if it resembles the schematic.\n",
        "\"\"\"\n",
        "# %%\n",
        "class Bert(nn.Module):\n",
        "    def __init__(self, config: BertConfig):\n",
        "        super().__init__()\n",
        "        pass\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None) -> BertOutput:\n",
        "        pass\n",
        "\n",
        "def test_bert(your_module):\n",
        "    \"\"\"Your full Bert should exactly match the reference solution in eval mode.\n",
        "\n",
        "    Dropout is not tested.\n",
        "    \"\"\"\n",
        "\n",
        "    config = BertConfig()\n",
        "    t.random.manual_seed(0)\n",
        "    reference = Bert(config)\n",
        "    reference.eval()\n",
        "    t.random.manual_seed(0)\n",
        "    theirs = your_module(config)\n",
        "    theirs.eval()\n",
        "    input_ids = t.LongTensor([[101, 1309, 6100, 1660, 1128, 1146, 102]])\n",
        "    allclose(theirs(input_ids=input_ids).logits, reference(input_ids=input_ids).logits)\n",
        "\n",
        "\n",
        "test_bert(Bert)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# %%\n",
        "\"\"\"\n",
        "# Loading Pretrained Weights \n",
        "Now copy parameters from the pretrained BERT returned by `load_pretrained_bert()` into your BERT.\n",
        "This is somewhat tedious, but is representative of real ML work. Race yourself to see if you can do it more quickly than last time!\n",
        "Remember that the embedding and unembedding weights are tied, so `hf_bert.bert.embeddings.word_embeddings.weight` and `hf_bert.cls.predictions.decoder.weight` should be equal and you should only use one of them.\n",
        "You can look at the solution if you get frustrated.\n",
        "<details>\n",
        "<summary>I'm confused about my Parameter not being a leaf!</summary>\n",
        "When you copied data from the HuggingFace version, PyTorch tracked the history of the copy operation. This means if you were to call `backward`, it would try to backpropagate through your Parameter back to the HuggingFace version, which is not what we want.\n",
        "To fix this, you can call `detach()` to make a new tensor that shares storage with the original doesn't have any history.\n",
        "</details>\n",
        "\"\"\"\n",
        "# %%\n",
        "def load_pretrained_weights(config: BertConfig) -> Bert:\n",
        "    hf_bert = load_pretrained_bert()\n",
        "    pass\n",
        "\n",
        "my_bert = load_pretrained_weights(config)\n",
        "\n",
        "for name, p in my_bert.named_parameters():\n",
        "    assert (\n",
        "        p.is_leaf\n",
        "    ), \"Parameter {name} is not a leaf node, which will cause problems in training. Try adding detach() somewhere.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# %%\n",
        "\"\"\"\n",
        "# Tokenization\n",
        "We're going to use a HuggingFace tokenizer for now to encode text into a sequence of tokens that our model can use. The tokenizer has to match the model - our model was trained with the `bert-base-cased` tokenizer which is case-sensitive. If you tried to use the `bert-base-uncased` tokenizer which is case-insensitive, it wouldn't work at all.\n",
        "Use `transformers.AutoTokenizer.from_pretrained` to fetch the appropriate tokenizer and try encoding and decoding some text.\n",
        "## Vocabulary\n",
        "Check out `tokenizer.vocab` to get an idea of what sorts of strings are assigned to tokens. In WordPiece, tokens represent a whole word unless they start with `##`, which denotes this token is part of a word. \n",
        "## Special Tokens\n",
        "Check out `tokenizer.special_tokens_map`. The strings here are mapped to tokens which have special meaning - for example `tokenizer.mask_token` which is the literal string '[MASK]' is converted to `tokenizer.mask_token_id` which is 103.\n",
        "## Predicting Masked Tokens\n",
        "Write the `predict` function which takes a string with one or more instances of the substring '[MASK]', runs it through your model, finds the top K predictions and decodes each prediction.\n",
        "Tips:\n",
        "- `torch.topk` is useful\n",
        "- The model should be in evaluation mode for predictions - this disables dropout and makes the predictions deterministic.\n",
        "- If your model gives different predictions than the HuggingFace section, proceed to the next section on debugging.\n",
        "\"\"\"\n",
        "# %%\n",
        "def predict(model: Bert, tokenizer, text: str, k=15) -> List[List[str]]:\n",
        "    \"\"\"\n",
        "    Return a list of k strings for each [MASK] in the input.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "def test_bert_prediction(predict, model, tokenizer):\n",
        "    \"\"\"Your Bert should know some names of American presidents.\"\"\"\n",
        "    text = \"Former President of the United States of America, George[MASK][MASK]\"\n",
        "    predictions = predict(model, tokenizer, text)\n",
        "    print(f\"Prompt: {text}\")\n",
        "    print(\"Model predicted: \\n\", \"\\n\".join(map(str, predictions)))\n",
        "    assert \"Washington\" in predictions[0]\n",
        "    assert \"Bush\" in predictions[0]\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "test_bert_prediction(predict, my_bert, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# now play with your BERT!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "your_text = \"The Answer to the Ultimate Question of Life, The Universe, and Everything is [MASK].\"\n",
        "predictions = predict(my_bert, tokenizer, your_text)\n",
        "print(\"Model predicted: \\n\", \"\\n\".join(map(str, predictions)))\n",
        "\n",
        "\n",
        "# %%\n",
        "\"\"\"\n",
        "# Model Debugging\n",
        "If your model works correctly at this point then congratulations, you can skip this section. \n",
        "The challenge with debugging ML code is that it often silently computes the wrong result instead of erroring out. Some things you can check:\n",
        "- Do I have any square matrices transposed, so the shapes still match but they do the wrong thing?\n",
        "- Did I forget to pass any optional arguments, and the wrong default is being used?\n",
        "- If I `print` my model, do the layers look right?\n",
        "- Can I add asserts in my code to check assumptions that I've made? In particular, sometimes unintentional broadcasting creates outputs of the wrong shape.\n",
        "You won't always have a reference implementation, but given that you do, a good technique is to use hooks to collect the inputs and outputs that should be identical, and compare when they start to diverge. This narrows down the number of places where you have to look for the bug.\n",
        "Read the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html) for `register_forward_hook` on a `nn.Module` and try logging the input and output of each block on your model and the HuggingFace version.\n",
        "\"\"\"\n",
        "# %%\n",
        "\n",
        "hf_bert = load_pretrained_bert()\n",
        "hf_bert.apply(remove_hooks)\n",
        "hf_bert.eval()\n",
        "# this should load in a BERT that can be used for reference and debugging"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
